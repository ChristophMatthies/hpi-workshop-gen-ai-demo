{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a4cac38-de55-4515-aa1f-7aa4e3ae78b8",
   "metadata": {},
   "source": [
    "## Coding challenge Part I (~45 min) - Using SAP's LLM Orchestration Service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283575ee-5bb1-4861-b87d-1d8b389b6135",
   "metadata": {},
   "source": [
    "This notebook will guide you in exploring some of SAP's generative AI capabilities around the LLM Orchestration Service.\n",
    "You will use Python and SAP's `generative-ai-hub-sdk` library to interact with LLMs and the Orchestration Service.\n",
    "\n",
    "(Note: SDKs are also available for JavaScript and Java) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92058657-ebe0-42e3-beed-ed0bde4a3e47",
   "metadata": {},
   "source": [
    "### SAP LLM Orchestration Service Overview\n",
    "\n",
    "![Orchestration Pipeline Modules](./img/orchestration_pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe27f9d-d58d-4247-8a75-3339c109a54a",
   "metadata": {},
   "source": [
    "**SAP LLM Orchestration Service: One API, multiple models, plus often-required business features**\n",
    "\n",
    "- Large Language Model (LLM) Access with the same API\n",
    "  - azure-openai (e.g. gpt-4o)\n",
    "  - opensource (e.g. Meta Llama3, Mistral AI Mixtral, IBM Granite)\n",
    "  - gcp-vertexai (e.g. Gemini 1.5 Flash)\n",
    "  - aws-bedrock (e.g. Anthropic Claude 3.5 Sonnet, Amazon Titan Text Lite)\n",
    "- Prompt Templating\n",
    "  - Enrich user prompts with additional context\n",
    "  - Reuse templates\n",
    "- Content Filtering\n",
    "  - Ensure that user input and model output adhere to safety standards\n",
    "- Data Masking\n",
    "  - Ensure that sensitive data is not transferred to third-party LLM providers\n",
    "- Grounding / Retrieval Augmented Generation\n",
    "  - Augment LLM prompts with related data fetched from specialized data sources the LLM would not have access to otherwise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d60b8aa-d530-4268-9eea-a6b28d3dffb6",
   "metadata": {},
   "source": [
    "### Let's get started: Preliminary steps\n",
    "- Please form groups of up to 5 people (maximum 5 groups in total).\n",
    "- Send an email (one per group) to christopher.hagedorn@sap.com to receive credentials file for the day.\n",
    "- Go through this notebook, run and understand the commands, and make LLM requests using Python and the SDK functions.\n",
    "  - You will have to fill in some parts to get things working correctly (TODO check if this is true).\n",
    "- Adjust parameters and play around with the functions as you like\n",
    "- If you get stuck or have any questions, approach us"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa98ff16-ad95-4c09-9117-6db800dbaf63",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "**1.** Verify that you have a supported and stable version of Python installed on your system\n",
    "* Open up a terminal session at the root of the cloned repository and check the output of: `python --version`\n",
    "\n",
    "**2.**  Setup a virtual Python environment to avoid conflicts with other potentially installed packages \n",
    "* Create a virtual environment: `python -m venv ENV`. Note: Your Python executable might also be called `python3`\n",
    "* Activate the environment in your current terminal session\n",
    "  * Unix-like systems, MacOS and Linux: `source ENV/bin/activate`\n",
    "  * Windows: run the script `ENV/Scripts/Activate.ps1`\n",
    "* Make sure that all subsequent steps are executed within the context of this newly created virtual environment\n",
    "  * your terminal line shows `(ENV)` at the very left-hand side\n",
    "\n",
    "**3.** Install the [SAP generative AI hub SDK](https://pypi.org/project/generative-ai-hub-sdk/)\n",
    "* `pip install \"generative-ai-hub-sdk[all]\"`. If you use this notebook, you can run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61e3f1c-c79d-492d-8cd5-f784a92d9da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \"generative-ai-hub-sdk[all]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4d2efc-d90b-4834-b1e3-de26a84556c5",
   "metadata": {},
   "source": [
    "**4.** Configure authentication by setting the following environment variables using the demo credentials. If you use this notebook, you can run either of the following cells.\n",
    "\n",
    "```bash\n",
    "AICORE_AUTH_URL=<workshop-credentials-file.AICORE_AUTH_URL>\n",
    "AICORE_BASE_URL=<workshop-credentials-file.AICORE_BASE_URL>\n",
    "AICORE_CLIENT_ID=<workshop-credentials-file.AICORE_CLIENT_ID>\n",
    "AICORE_CLIENT_SECRET=<workshop-credentials-file.AICORE_CLIENT_SECRET>\n",
    "AICORE_ORCHESTRATION_DEPLOYMENT_URL=<workshop-credentials-file.AICORE_ORCHESTRATION_DEPLOYMENT_URL>\n",
    "AICORE_RESOURCE_GROUP=<workshop-credentials-file.AICORE_RESOURCE_GROUP>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6928ef2d-dc42-4d5a-90ce-932e016d4de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Place the file shared with you into a folder tmp from the root directory\n",
    "#       copy the filename and fill the placeholder <workshop-credentials-file> with it in the last line\n",
    "%pip install python-dotenv\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "# loading variables from file\n",
    "load_dotenv(\"tmp/dummy.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f4a6f0-af7a-4d2f-a9ce-6a686908d315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT NEEDED if you have executed the cell above\n",
    "# TODO: Copy the each environment variable's content from the file shared via email with you,\n",
    "#       replacing the appropriate placeholder \"<workshop-credentials-file.xyz>\"\n",
    "import os\n",
    "os.environ[\"AICORE_AUTH_URL\"] = \"<workshop-credentials-file.AICORE_AUTH_URL>\"\n",
    "os.environ[\"AICORE_BASE_URL\"] = \"<workshop-credentials-file.AICORE_BASE_URL>\"\n",
    "os.environ[\"AICORE_CLIENT_ID\"] = \"<workshop-credentials-file.AICORE_CLIENT_ID>\"\n",
    "os.environ[\"AICORE_CLIENT_SECRET\"] = \"<workshop-credentials-file.AICORE_CLIENT_SECRET>\"\n",
    "os.environ[\"AICORE_ORCHESTRATION_DEPLOYMENT_URL\"] = \"<workshop-credentials-file.AICORE_ORCHESTRATION_DEPLOYMENT_URL>\"\n",
    "os.environ[\"AICORE_RESOURCE_GROUP\"] = \"<workshop-credentials-file.AICORE_RESOURCE_GROUP>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465e86c1-1e00-4583-a803-654f7082af5e",
   "metadata": {},
   "source": [
    "**5.** You can now open your preferred development environment for Python and start with the exercises. Ensure that the created virtual environment and the set environment variables are used within your development environment.\n",
    "\n",
    "We recommend using Jupyter Notebook to explore this tracks exercises. You can install it within your virtual environment using `pip install notebook`. Afterwards you can use the following command from your repository's root to view and edit the Jupyter Notebooks provided in this track: `jupyter notebook exercises/python/`. A browser window should open up automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6b0933-8cd2-4641-86f5-463b72187073",
   "metadata": {},
   "source": [
    "### Let's get started: Preliminary steps\n",
    "Go through the subsequent cells to familiarise yourself with the SAP LLM Orchestration Service and how the `generative-ai-hub-sdk` supports you in using the SAP LLM Orchestration Service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb85475-dc4a-48e7-8410-c95a5081dd62",
   "metadata": {},
   "source": [
    "#### Create Prompt Template\n",
    "The first module as part of the SAP LLM Orchestration Service we will use is templating. \n",
    "The templating module provides capabilities to define prompt skeletons that can then be parameterized per inference call.\n",
    "In the following we can define such a template.\n",
    "A template consists of \n",
    "- *messages*: a sequence of messages, which can include templating syntax\n",
    "- *defaults*: an optional list of defaults for introduced template parameters\n",
    "\n",
    "The messages can be of different type:\n",
    "- *SystemMessage*: A message for priming AI behavior. The system message is usually passed in as the first of a sequence of input messages.\n",
    "- *UserMessage*: A message from a user.\n",
    "- *AssistantMessage*: A message of the LLM.\n",
    "\n",
    "Template parameters are defined within the message string using the following syntax: `{{?param_name}}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48b428b-f161-459a-b216-efd297f1e1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please fill in the commented sections of this cell and execute it.\n",
    "from gen_ai_hub.orchestration.models.message import SystemMessage, UserMessage\n",
    "from gen_ai_hub.orchestration.models.template import Template, TemplateValue\n",
    "\n",
    "support_template = Template(\n",
    "    messages=[\n",
    "        SystemMessage(\"You are a helpful AI assistant that specializes in humorously answering business questions by users of SAP software. Every answer you give should end with a funny exclamation. Rhyming is an absolute must and a hard requirement!\"),\n",
    "        UserMessage(\n",
    "            # TODO add template parameters in the below sentence replacing the ...\n",
    "            \"Please answer with no more than ... sentences and include ... emojis. The user asks the following: {{?user_question}}\"\n",
    "        ),\n",
    "    ],\n",
    "    defaults=[\n",
    "        TemplateValue(name=\"user_question\", value=\"\"),\n",
    "        # TODO add additional TemplateValues here according to the ones filled above\n",
    "        # ...,\n",
    "        # ...\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13e54e2-9d88-4593-b6dc-f68e48cfa002",
   "metadata": {},
   "source": [
    "#### Define the LLM\n",
    "\n",
    "Select a Large Language Model (LLM) that will be used for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b320618-d310-4916-9cc3-705904486b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gen_ai_hub.orchestration.models.llm import LLM\n",
    "\n",
    "gpt_4o_llm = LLM(name=\"gpt-4o\", version=\"latest\", parameters={\"max_tokens\": 256, \"temperature\": 0.2})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17304bd1-242d-4a6b-994e-2302608c3024",
   "metadata": {},
   "source": [
    "#### Create the Orchestration Configuration\n",
    "\n",
    "Now create an orchestration config. The config is the central object that contains information about the modules used during inference run. For now, you will need to add the template and the llm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cd9188-0fd8-4864-9e01-4caa0e8a033b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please fill in the commented sections of this cell and execute it.\n",
    "from gen_ai_hub.orchestration.models.config import OrchestrationConfig\n",
    "\n",
    "config = OrchestrationConfig(\n",
    "    #template=<add template to this call>,\n",
    "    #llm=<add llm to this call>\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23137afa-d6c7-4f1a-9280-ff1ca5b427d1",
   "metadata": {},
   "source": [
    "#### Run the Orchestration Request\n",
    "\n",
    "Now we want to see the orchestration service in action to summarize a prepared cv file, which is stored in `data/cv.txt`.\n",
    "In preparation, we first create an orchestration service with the orchestration deployment url that you have been provided `\"AICORE_ORCHESTRATION_DEPLOYMENT_URL\"`. Second we load the cv text from the file.\n",
    "\n",
    "Lastly, we can call the orchestration service. Note that the actual template values are now passed to the run method. The TemplateValue name parameter corresponds to the parameter name text provided in the user message string. The parameter to_lang is omitted and the default defined in the PromptTemplate is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15200b45-65bd-44b8-b12b-28d5eaa0c77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gen_ai_hub.orchestration.service import OrchestrationService\n",
    "\n",
    "service = OrchestrationService(api_url=os.environ[\"AICORE_ORCHESTRATION_DEPLOYMENT_URL\"], config=config)\n",
    "\n",
    "# TODO add your question\n",
    "q = \"\"\n",
    "\n",
    "\n",
    "result = service.run(\n",
    "    config=config,\n",
    "    template_values=[\n",
    "        TemplateValue(name=\"user_question\", value=q)\n",
    "    ]\n",
    ")\n",
    "print(result.orchestration_result.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7cf2f5-8c68-46a0-9909-9f73aaa787fb",
   "metadata": {},
   "source": [
    "#### Adapt Default Template Values\n",
    "\n",
    "Now that you have successfully posed your first question using some default parameters. Let's adapt the some of the defaults in the next call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f6b382-0c3c-4487-baa8-ef46d0c0fb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please adjust the other two templateValues that you set defaults for.\n",
    "template_values = [\n",
    "    TemplateValue(name=\"user_question\", value=q)\n",
    "    # Add TemplateValue here,\n",
    "    # Add TemplateValue here\n",
    "]\n",
    "result = service.run(template_values=template_values)\n",
    "print(result.orchestration_result.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f256d7de-5260-4844-8823-d621613759ac",
   "metadata": {},
   "source": [
    "#### Adapt Model Parameters\n",
    "\n",
    "Model parameters are settings that influence how LLM output is created.\n",
    "\n",
    "Potentially most interesting is the `temperature` setting. Learn about it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c61b35-fc63-42bf-8e3c-c75c7837ba28",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = service.run(template_values=[\n",
    "    TemplateValue(\n",
    "        name=\"user_question\",\n",
    "        value=\"What is the `temperature` in LLM configurations? What difference does it make?\")\n",
    "])\n",
    "print(result.orchestration_result.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf608df-fde7-47b1-b6e3-0c2e3d407aa5",
   "metadata": {},
   "source": [
    "Let's crank up the heat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5bf537-863d-4f8b-aa92-9f38b48411db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO change the temperature parameter of the LLM. Use a value between 0.0 to 2.0, try out a high value.\n",
    "\n",
    "# temperature \n",
    "\n",
    "hot_gpt_4o_llm = LLM(name=\"gpt-4o\",\n",
    "                     version=\"latest\", \n",
    "                     parameters={\n",
    "                         \"max_tokens\": 256, \n",
    "                         \"temperature\": temperature}\n",
    "                    )\n",
    "\n",
    "result = service.run(\n",
    "    config=OrchestrationConfig(template=support_template, llm=hot_gpt_4o_llm),\n",
    "    template_values=template_values\n",
    ")\n",
    "\n",
    "print(result.orchestration_result.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2aba9dd-eec6-44fa-9b4b-060303d4fa1d",
   "metadata": {},
   "source": [
    "#### Adapt the Model Being Called\n",
    "\n",
    "We've been using OpenAI's GPT-4o model. The LLM Orchestration service offers other options, examples are provided in the cell below. For a list of supported models see this [link](https://help.sap.com/doc/generative-ai-hub-sdk/CLOUD/en-US/_reference/README_sphynx.html#supported-models). Note that in our test environment it is not guaranteed that all of these models work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a345026-e0a3-4e20-8a30-2b1abd20f4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini = LLM(name=\"gemini-1.5-flash\", version=\"latest\", parameters={\"max_tokens\": 256, \"temperature\": 1.0})\n",
    "claude_3_haiku = LLM(name=\"anthropic--claude-3-haiku\", version=\"latest\", parameters={\"max_tokens\": 256, \"temperature\": 1.0})\n",
    "titan_text = LLM(name=\"amazon--titan-text-lite\", version=\"latest\", parameters={\"max_tokens\": 256, \"temperature\": 1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9997fda5-4270-415c-b867-3b8a2a1aa595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO change below call to use different llm \n",
    "\n",
    "result = service.run(\n",
    "    config=OrchestrationConfig(template=support_template, llm=hot_gpt_4o_llm),\n",
    "    template_values=template_values\n",
    ")\n",
    "print(result.orchestration_result.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205fa535-8eca-4186-bf73-ef0a9ddb1a43",
   "metadata": {},
   "source": [
    "#### Compare Different LLMs\n",
    "Let us consider an easy mathematical problem that is taken from the following research paper \"Easy Problems That LLMs Get Wrong: https://arxiv.org/abs/2405.19616\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9bd199-8ad1-40a8-a70e-b4eb1161b949",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Template(messages=[\n",
    "    SystemMessage(\"Only answer with the shortest possible response!\"),\n",
    "    UserMessage(\"Count the number of occurrences of the letter ’L’ in the word ’LOLLAPALOOZA’\")\n",
    "])\n",
    "\n",
    "models_to_test = [\"gemini-1.5-flash\", \"anthropic--claude-3-haiku\", \"amazon--titan-text-lite\", \"ibm--granite-13b-chat\", \"meta--llama3-70b-instruct\"]\n",
    "version = \"latest\"\n",
    "parameters={\"max_tokens\": 256, \"temperature\": 0.1}\n",
    "\n",
    "# TODO iterate the models_to_test and create a new llm for each instance with the version and parameters provided above.\n",
    "#      for each of the llms make an orchestration call with the template provided above and print the result using the following code\n",
    "#      result = service.run(config=OrchestrationConfig(template=<TODO replace>, llm=<TODO replace>)\n",
    "#      print(f\"--- {<TODO print name of the model to test>} ---\")\n",
    "#      print(result.orchestration_result.choices[0].message.content.strip('\\n'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9e1f19-571d-4a9b-bd05-bca0906691a0",
   "metadata": {},
   "source": [
    "#### Building A Feature using a LLM\n",
    "\n",
    "One of the most powerful features of LLMs is their language understanding. LLMs make surprisingly good translators.\n",
    "Create a `Translator` class that contains an orchestration service and an orchestration service. The orchestration service is provided as an input during initilization. The orchestration config should contain an llm (of your choice) and a template with System and UserMessages that allow to provide a target language and a text as parameters.\n",
    "This class should have a function `translate()` that takes a text and a target language as input. These inputs should be used to update the template parameters when making a call to the orchestration service. Ensure that the response is returned to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c727f0-c481-48b4-b872-0a5666b61771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO fill the code snippets below to create the translator\n",
    "class Translator:\n",
    "    def __init__(self, orch_service):\n",
    "        self.service = None #TODO complete me!\n",
    "        self.config = OrchestrationConfig() #TODO complete me!\n",
    "\n",
    "    def translate(self, text, to_lang=\"Spanish\"):\n",
    "        response = self.service.run(\n",
    "            config=self.config,\n",
    "            # TODO add template values\n",
    "        )\n",
    "\n",
    "        return response.orchestration_result.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104eca3f-1a55-45c5-a64b-305c2eb778c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator(orch_service=service)\n",
    "result = translator.translate(text=\"Hello, world!\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dd3279-6e3a-43d8-8ced-37f04502966a",
   "metadata": {},
   "source": [
    "#### Stretch Goal: Using Content Filtering\n",
    "\n",
    "As a Business AI Developer, we must ensure that the content being translated by users as part of our software solutions meets certain safety standards.\n",
    "\n",
    "We can use the Orchestration Service's content filtering features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47737ce-f432-4bcb-909c-1ba0f35fbf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gen_ai_hub.orchestration.models.azure_content_filter import AzureContentFilter, AzureThreshold\n",
    "\n",
    "# define a content filter with settings for the four categotries hate, violence, seflharm and sexual\n",
    "input_filter= AzureContentFilter(hate=AzureThreshold.ALLOW_SAFE,\n",
    "                                 violence=AzureThreshold.ALLOW_SAFE,\n",
    "                                 self_harm=AzureThreshold.ALLOW_SAFE,\n",
    "                                 sexual=AzureThreshold.ALLOW_SAFE)\n",
    "\n",
    "# apply this content filter to the OrchestrationConfig\n",
    "config_w_filter = OrchestrationConfig(\n",
    "    template=Template(\n",
    "        messages=[\n",
    "            UserMessage(\"{{?text}}\")\n",
    "        ]\n",
    "    ),\n",
    "    llm=LLM(name=\"gpt-4o\",),\n",
    "    input_filters=[input_filter]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b279aa31-e8bd-44da-8e46-6a0a0f3c16c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO add error handling (try / except ) to the following call to handle the raised exeception. Print the error message.\n",
    "from gen_ai_hub.orchestration.exceptions import OrchestrationError\n",
    "\n",
    "# when the service filters content it raises an exception\n",
    "result = service.run(\n",
    "    config=config_w_filter,\n",
    "    template_values=[\n",
    "        TemplateValue(name=\"text\", value=\"I hate you\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bd6ea2-18fc-4fe6-972b-133d2210537e",
   "metadata": {},
   "source": [
    "#### Stretch Goal: Adding Content Filtering to the Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df9eb3d-b500-4d4e-9ccc-98bcd0960e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO implement a translator that refuses to translate non-safe content\n",
    "class SafeTranslator:\n",
    "    def __init__(self, orch_service):\n",
    "        #self.service = None\n",
    "        # --- Set a config with filtering ---\n",
    "        self.config = OrchestrationConfig() #TODO complete me!\n",
    "\n",
    "    def translate(self, text, to_lang=\"French\"):\n",
    "        response = self.service.run(\n",
    "            config=self.config,\n",
    "            # TODO add template values\n",
    "        )\n",
    "\n",
    "        return response.orchestration_result.choices[0].message.content\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
